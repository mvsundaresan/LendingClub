{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a712a54f-2a3f-4b6c-afec-e6b9a63b70a8",
   "metadata": {},
   "source": [
    "# Lending Club Analysis\n",
    "## Goal: Maximize profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f430df-f7d3-4b0f-b374-1244c3d4f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements: pandas, numpy, scikit-learn, xgboost, shap\n",
    "# !pip install pandas numpy scikit-learn xgboost shap seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b8b77-d99d-47aa-9551-c0e1c902bb2b",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad433b-acb7-4e48-b285-99ac253da073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (8,5), \"font.size\": 11})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Output folder\n",
    "OUT = Path.cwd() / \"lc_outputs\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CSV path\n",
    "CSV_PATH = Path.cwd() / \"Lending Club Data - DR_Demo_Lending_Club.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38773b2-53ed-489d-aa08-b4069e5e6e07",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36db1460-f762-47e0-842b-d5744b1cb0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========\n",
    "# Load data\n",
    "# ==========\n",
    "print(\"Loading:\", CSV_PATH)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Rows:\", df.shape[0])\n",
    "print(\"Columns:\", len(df.columns))\n",
    "\n",
    "# quick peek\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce2f3f-ec7d-4a12-8795-288d577ade97",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d896db8-ef1a-4f1c-ab97-9c8ab969f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98453a64-feb0-49c7-93e1-2e847a72ce55",
   "metadata": {},
   "source": [
    " #   Column                       Description\n",
    "---  ------                       --------------------------\n",
    " 0   Id                           Unique Identifier (Exclude)\n",
    " 1   is_bad                       Target Column\n",
    " 2   emp_title                    Borrower's employer / designation (Exclude - High Cardinality)\n",
    " 3   emp_length                   Borrower's employment tenure \n",
    " 4   home_ownership               Borrower own a home or rent\n",
    " 5   annual_inc                   Borrower's annual income \n",
    " 6   verification_status          Borrower's income verified or not\n",
    " 7   pymnt_plan                   Borrower setup a payment plan or not\n",
    " 8   Notes                        Free form text about the loan (Exclude)\n",
    " 9   purpose_cat                  Loan purpose category\n",
    " 10  purpose                      Loan purpose (Exclude - High Cardinality)\n",
    " 11  zip_code                     Zipcode (Exclude - High Cardinality)\n",
    " 12  addr_state                   State Abbreviation (Exclude - High Cardinality)\n",
    " 13  debt_to_income               Debt-to-Income Ratio\n",
    " 14  delinq_2yrs                  Delinquent in the past 2 years (Past Due)\n",
    " 15  earliest_cr_line             Earliest credit line opened (Year)\n",
    " 16  inq_last_6mths               Recent inquiries\n",
    " 17  mths_since_last_delinq       Recent delinquency\n",
    " 18  mths_since_last_record       Recent public record (Exclude - > 95% missing values)\n",
    " 19  open_acc                     Number of open accounts\n",
    " 20  pub_rec                      Number of public records (bankruptcies, liens, judgments)\n",
    " 21  revol_bal                    Revolving balance - Debt Load\n",
    " 22  revol_util                   Utilization Rate (in %)\n",
    " 23  total_acc                    Total number of accounts\n",
    " 24  initial_list_status          Not sure what this column contains (Exclude/Opetional)\n",
    " 25  collections_12_mths_ex_med   Recent collections (Excl. Med) (Exclude all zeros)\n",
    " 26  mths_since_last_major_derog  Recent major derogatory\n",
    " 27  policy_code                  Not sure what this column contains (Exclude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7363c0d5-6d9d-44bb-9539-71fbf246d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0177f67-3947-4056-a0af-26c2d41a5037",
   "metadata": {},
   "source": [
    "## Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896afd4-250d-4e21-a541-e94b536fd7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"is_bad\"\n",
    "\n",
    "print(\"Using target:\", target_col)\n",
    "print(\"Target distribution:\")\n",
    "print(df[target_col].value_counts(dropna=False, normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727062a5-ae47-454a-ab63-fad1ed89d433",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ff18f-8268-4601-b34b-4bc93b68530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Basic cleaning & drop columns\n",
    "# ======================================\n",
    "# Drop obvious ID/text columns not useful for modeling\n",
    "drop_candidates = [\"Id\",\"emp_title\",\"Notes\",\"purpose\",\"zip_code\",\"addr_state\",\"mths_since_last_record\",\n",
    "                   \"initial_list_status\",\"collections_12_mths_ex_med\",\"policy_code\"]\n",
    "\n",
    "print(\"Dropping:\", drop_candidates)\n",
    "df = df.drop(columns=drop_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545bc320-5b8e-4d21-89db-846b9be0095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Convert date into age\n",
    "# ======================\n",
    "# Ensure column is in datetime format\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%m/%d/%y')\n",
    "\n",
    "# Calculate age in years\n",
    "df['credit_age_years'] = (datetime.now() - df['earliest_cr_line']).dt.days / 365.25\n",
    "\n",
    "# Handle invalid or missing dates\n",
    "df['credit_age_years'] = df['credit_age_years'].fillna(df['credit_age_years'].median())\n",
    "\n",
    "# Check results\n",
    "print(df[['earliest_cr_line', 'credit_age_years']].head())\n",
    "\n",
    "# Convert the emp_length to numeric\n",
    "df['emp_length'] = pd.to_numeric(df['emp_length'], errors='coerce')\n",
    "print(df['emp_length'].value_counts())\n",
    "\n",
    "# Select all object-typed columns\n",
    "object_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Convert selected columns to 'category' dtype\n",
    "for col in object_cols:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726b34c-6052-478d-9adf-122857866ac5",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c510fb5-b3bc-44c8-b386-a843cf8788a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# Feature selection  \n",
    "# ==================\n",
    "\n",
    "# Choose numeric and categorical features for modeling,\n",
    "NUMERIC = df.select_dtypes(include=[\"int64\",\"float64\"]).columns.tolist()\n",
    "NUMERIC = [c for c in NUMERIC if c != target_col]  # exclude target\n",
    "\n",
    "CATEGORICAL = df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "# limit categorical to those with <= 50 unique values (adjustable)\n",
    "CATEGORICAL = [c for c in CATEGORICAL if df[c].nunique() <= 50]\n",
    "\n",
    "# Remove any columns with > 95% missing\n",
    "cols_to_drop = [c for c in df.columns if df[c].isna().mean() > 0.95]\n",
    "print(\"Drop columns with most missing values:\", cols_to_drop)\n",
    "if cols_to_drop:\n",
    "    print(\"Dropping >95% missing:\", cols_to_drop)\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    NUMERIC = [c for c in NUMERIC if c not in cols_to_drop]\n",
    "    CATEGORICAL = [c for c in CATEGORICAL if c not in cols_to_drop]\n",
    "\n",
    "print(\"Numeric features chosen:\", len(NUMERIC))\n",
    "print(\"Categorical features chosen:\", len(CATEGORICAL))\n",
    "\n",
    "# keep final feature lists (you can customize manually)\n",
    "FEATURES = NUMERIC + CATEGORICAL\n",
    "print(\"Total features to use:\", len(FEATURES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf36186-4df6-4234-a2e4-30b55fc0ea91",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a871e-023a-4b61-90a2-6c553765956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Exploratory Data Analysis (EDA) \n",
    "# ================================\n",
    "# Basic distributions\n",
    "print(\"\\n--- EDA summary ---\")\n",
    "print(\"Default rate:\", df[target_col].mean())\n",
    "\n",
    "# Missingness overview (top 15)\n",
    "missing = df[FEATURES + [target_col]].isna().sum().sort_values(ascending=False)\n",
    "print(\"Top missing counts:\\n\", missing.head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa5698-05ab-4a20-aea4-f5d32bf85815",
   "metadata": {},
   "source": [
    "### Numerical Features - Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65063c-c96e-4f61-a99d-c10dda703895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.countplot(x=target_col, data=df)\n",
    "plt.title(\"Target distribution (0 = good, 1 = default)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Numeric histograms\n",
    "for col in NUMERIC:\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.histplot(df[col].dropna(), bins=40, kde=False)\n",
    "    plt.title(f\"Histogram: {col}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e6bd5f-f9c2-4e61-b37c-e9a22012917b",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ec836-ac11-496d-9eb4-09d72a02d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap of numeric features\n",
    "if len(NUMERIC) >= 2:\n",
    "    num_corr = df[NUMERIC].corr().abs()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(df[NUMERIC + [target_col]].corr(), cmap=\"coolwarm\", center=0, annot=True)\n",
    "    plt.title(\"Numeric Features - Correlations\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    # plt.yticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc7b60-b6e2-41de-87c6-613dd691d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUMERIC:\n",
    "    corr_with_target = df[NUMERIC].corrwith(df[target_col]).sort_values(ascending=False)\n",
    "\n",
    "corr_with_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869cdd1f-29b2-466f-9a79-993bfb15e253",
   "metadata": {},
   "source": [
    "### Numerical Features - Binned Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d4e4d-b8f4-4d39-8e33-a00fa7d5ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric feature vs target: binned default rate\n",
    "for col in NUMERIC:\n",
    "    tmp = df[[col, target_col]].dropna()\n",
    "    tmp[\"bin_index\"] = pd.qcut(tmp[col], q=10, duplicates=\"drop\")\n",
    "    br = tmp.groupby(\"bin_index\")[target_col].mean()\n",
    "    plt.figure(figsize=(6,4))\n",
    "    br.plot(kind=\"bar\")\n",
    "    plt.title(f\"Default rate by {col} decile\")\n",
    "    plt.ylabel(\"Default rate\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11662f3-98af-4e84-a20a-7d194ae0d19c",
   "metadata": {},
   "source": [
    "### Categorical Features - Top 10 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90419376-2567-4487-af8e-962c85cb00d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical vs target (top categories)\n",
    "for col in CATEGORICAL:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    order = df[col].value_counts(ascending=False).index[:10]\n",
    "    sns.barplot(x=col, y=target_col, data=df, order=order)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(f\"Default rate by {col} (top categories)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed449c0-2f15-48c8-ad03-1ca11d9a111c",
   "metadata": {},
   "source": [
    "## Address Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3a2ba-28b5-4a16-9d71-910e87582f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# filter classes\n",
    "majority = df[df['is_bad'] == 0] # Negative class - Good \n",
    "minority = df[df['is_bad'] == 1] # Positive class - Default\n",
    "\n",
    "# Set target size (e.g., average of both classes)\n",
    "target_size = int((len(majority) + len(minority)) / 2)\n",
    "\n",
    "# Downsample majority class\n",
    "majority_downsampled = resample(majority,\n",
    "                                replace=False,\n",
    "                                n_samples=target_size,\n",
    "                                random_state=42)\n",
    "\n",
    "# Upsample minority class\n",
    "minority_upsampled = resample(minority,\n",
    "                              replace=True,\n",
    "                              n_samples=target_size,\n",
    "                              random_state=42)\n",
    "\n",
    "# Combine into a new DataFrame\n",
    "balanced_df = pd.concat([majority_downsampled, minority_upsampled])\n",
    "\n",
    "# Shuffle the dataset (optional)\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(balanced_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56a43c-5f73-4b96-b262-3c89733601b6",
   "metadata": {},
   "source": [
    "### Verify Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480d907-f1e6-4ce0-ac28-ced1b3ef65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using target:\", target_col)\n",
    "print(\"Target distribution:\")\n",
    "print(balanced_df[target_col].value_counts(dropna=False, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698a654-86ff-41ac-96c5-f3b00f22f281",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0798e6-9210-4867-83e4-4339a4bc82ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# numeric pipeline\n",
    "num_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# categorical pipeline\n",
    "cat_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"MISSING\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", dtype=np.float32))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_transformer, NUMERIC),\n",
    "        (\"cat\", cat_transformer, CATEGORICAL),\n",
    "    ],\n",
    "    remainder=\"drop\", verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# Fit-transform once to get output feature dimension\n",
    "X = balanced_df[FEATURES].copy()\n",
    "y = balanced_df[target_col].astype(int)\n",
    "\n",
    "preprocessor.fit(X)\n",
    "print(\"Preprocessor fitted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fffef7-3a07-4ca1-ad88-8165ad43fcf4",
   "metadata": {},
   "source": [
    "## Split the dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6baae5-6558-488f-aee5-9fb8b68af3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Preprocessing pipelines & train/test split\n",
    "# ===========================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "print(\"Train / Test:\", X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c26c945-4cc3-494c-9330-e7c007e2968a",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66acde90-3e58-4797-afd1-d8570b6554cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# Model training (Logistic Regression, RandomForest, XGBoost/GradBoost)\n",
    "# ======================================================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.pipeline import Pipeline as ImbPipeline \n",
    "\n",
    "models = {}\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Logistic Regression (balanced)\n",
    "pipe_lr = Pipeline([\n",
    "    (\"preproc\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", solver=\"saga\"))\n",
    "])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "models[\"LogisticRegression\"] = pipe_lr\n",
    "print(\"Trained LogisticRegression\")\n",
    "\n",
    "# Random Forest\n",
    "pipe_rf = Pipeline([\n",
    "    (\"preproc\", preprocessor),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=1000, random_state=42, class_weight=\"balanced\", n_jobs=-10))\n",
    "])\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "models[\"RandomForest\"] = pipe_rf\n",
    "print(\"Trained RandomForest\")\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "pipe_xgb = Pipeline([\n",
    "    (\"preproc\", preprocessor),\n",
    "    (\"clf\", XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42, n_estimators=1000, n_jobs=10))\n",
    "])\n",
    "pipe_xgb.fit(X_train, y_train)\n",
    "models[\"XGBoost\"] = pipe_xgb\n",
    "print(\"Trained XGBoost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377e277-042a-420b-87ca-8a99d77df059",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e47bd-5418-4d59-bfe2-479bb32f23b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Evaluation helper & results table\n",
    "# ==================================\n",
    "def evaluate_model(pipe, X_test, y_test):\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_proba = pipe.predict_proba(X_test)[:,1]\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return {\"accuracy\":acc, \"precision\":prec, \"recall\":rec, \"f1\":f1, \"auc\":auc, \"confusion_matrix\":cm.tolist(), \"y_proba\":y_proba, \"y_pred\":y_pred}\n",
    "\n",
    "results_list = []\n",
    "for name, pipe in models.items():\n",
    "    print(\"Evaluating:\", name)\n",
    "    res = evaluate_model(pipe, X_test, y_test)\n",
    "    res_row = {\"model\": name, \"accuracy\": res[\"accuracy\"], \"precision\": res[\"precision\"],\n",
    "               \"recall\": res[\"recall\"], \"f1\": res[\"f1\"], \"auc\": res[\"auc\"], \"confusion_matrix\": res[\"confusion_matrix\"]}\n",
    "    results_list.append(res_row)\n",
    "    \n",
    "results_df = pd.DataFrame(results_list).sort_values(by=\"f1\", ascending=False)\n",
    "display(results_df)\n",
    "results_df.to_csv(OUT / \"model_comparison.csv\", index=False)\n",
    "print(\"Saved model_comparison.csv\")\n",
    "\n",
    "# Plot ROC curves if probabilities available\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "plt.figure()\n",
    "for name, pipe in models.items():\n",
    "    y_proba = pipe.predict_proba(X_test)[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc:.3f})\")\n",
    "\n",
    "plt.plot([0,1],[0,1],\"k--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curves\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT / \"roc_curves.png\")\n",
    "plt.show()\n",
    "print(\"Saved AUC-ROC curve plot - roc_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046e36c-0faa-4515-9022-675f05cb3c59",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9594d-3ee7-48ca-bab8-c805401d4fe6",
   "metadata": {},
   "source": [
    "### Logistic Regression Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1943d1-a334-421c-a5dc-80c3f528f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the classifier from the pipeline\n",
    "lr_clf = models['LogisticRegression'].named_steps['clf']\n",
    "\n",
    "# Get coefficients\n",
    "feature_coefficients = lr_clf.coef_[0] \n",
    "\n",
    "# Get feature names\n",
    "feature_names = models['LogisticRegression'][:-1].get_feature_names_out()\n",
    "\n",
    "feature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': feature_coefficients})\n",
    "# feature_importance['Absolute_Coefficient'] = abs(feature_importance['Coefficient'])\n",
    "feature_importance = feature_importance.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = np.argsort(feature_coefficients)\n",
    "sorted_importances = feature_coefficients[sorted_idx]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(sorted_importances)), sorted_importances, tick_label=sorted_feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Logistic Regression Feature Importances\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20793d03-4f07-4945-9c44-737ec9011b3a",
   "metadata": {},
   "source": [
    "### RandomForest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1be6d-e5e9-4ac3-ab0f-799de37a779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the classifier from the pipeline\n",
    "rf_clf = models['RandomForest'].named_steps['clf']\n",
    "\n",
    "# get feature importances\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "\n",
    "# feature names\n",
    "feature_names = models['RandomForest'][:-1].get_feature_names_out()\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = np.argsort(feature_importances)\n",
    "sorted_importances = feature_importances[sorted_idx]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(sorted_importances[:20])), sorted_importances[:20], tick_label=sorted_feature_names[:20])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Random Forest Feature Importances\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3447b0-21c3-4854-ab40-f9d83e8e2912",
   "metadata": {},
   "source": [
    "### XGBoost Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d7175f-d760-4bf5-9264-235ebbc9ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the classifier from the pipeline\n",
    "xgb_clf = models['XGBoost'].named_steps['clf']\n",
    "\n",
    "# get feature importances\n",
    "feature_importances = xgb_clf.feature_importances_\n",
    "\n",
    "# feature names\n",
    "feature_names = models['XGBoost'][:-1].get_feature_names_out()\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = np.argsort(feature_importances)\n",
    "sorted_importances = feature_importances[sorted_idx]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(sorted_importances[:20])), sorted_importances[:20], tick_label=sorted_feature_names[:20])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"XGBoost Feature Importances\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c289a8-8379-4ed2-8667-a3bec00c1cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
